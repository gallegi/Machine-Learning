{"cells":[{"cell_type":"markdown","source":["## TensorFrames: Quick Start\nThis notebook provides a TensorFrames Quick Start using Databricks Community Edition.  You can run this from the `pyspark` shell like any other Spark package:\n\n```\n# The version we're using in this notebook\n$SPARK_HOME/bin/pyspark --packages tjhunter:tensorframes:0.2.2-s_2.10  \n\n# The latest version \n$SPARK_HOME/bin/pyspark --packages databricks:tensorframes:0.2.3-s_2.10\n```\n\nFor more information, please refer to the [databricks/tensorframes](https://github.com/databricks/tensorframes) github repo."],"metadata":{}},{"cell_type":"markdown","source":["## Configuration and Setup\nPlease follow the configuration and setup steps below in the following order:\n1. Launch a Spark cluster using **Spark 1.6 (Hadoop 1)** and **Scala 2.10**\n * This has been tested with Spark 1.6, Spark 1.6.2, and Spark 1.6.3 (Hadoop 1) on [Databricks Community Edition](http://databricks.com/try-databricks)\n2. Attach to this cluster TensorFrames 0.2.2: `tensorframes-0.2.2-s_2.10`\n3. In a notebook, run *one* of the following command to install [TensorFlow](https://www.tensorflow.org/).  This has been tested with TensorFlow 0.9 CPU edition (0.9 installs a bit faster) \n * *TensorFlow 0.9, Ubuntu/Linux 64-bit, CPU only, Python 2.7*\n `/databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl`\n * *TensorFlow 0.9, Ubuntu/Linux 64-bit, GPU enabled, Python 2.7*\n `/databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl`\n4. Detach and reattach the notebook you just ran this command from\n5. Your cluster is now configured. You can run pure tensorflow programs on the driver, or TensorFrames examples on the whole cluster"],"metadata":{}},{"cell_type":"markdown","source":["#### Install TensorFlow\n* Remember to first install TensorFrames 0.2.2: `tensorframes-0.2.2-s_2.10`\n* This is the `pip install` command that will install `TensorFlow` on to the Apache Spark driver"],"metadata":{}},{"cell_type":"code","source":["%sh\n/databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Detach and re-attach this notebook from the cluster\n* Once you have done this, you will be able to run `TensorFlow` programs on the driver and `TensorFrames` programs on the cluster."],"metadata":{}},{"cell_type":"markdown","source":["## Quick Start\nThis is a simple TensorFrames program that where the `op` is to perform a simple addition.  The original source code can be found at the [databricks/tensorframes](https://github.com/databricks/tensorframes) GitHub repo. This is in reference to the TensorFrames Readme.md > [How to Run in Python](https://github.com/databricks/tensorframes#how-to-run-in-python) section.\n\n\n### Use Tensorflow to add 3 to an existing column\nThe first thing we will do is import TensorFlow, TensorFrames, and pyspark.sql.row and create a dataframe based on an RDD of floats."],"metadata":{}},{"cell_type":"code","source":["# Import TensorFlow, TensorFrames, and Row\nimport tensorflow as tf\nimport tensorframes as tfs\nfrom pyspark.sql import Row\n\n# Create RDD of floats and convert into DataFrame `df`\nrdd = [Row(x=float(x)) for x in range(10)]\ndf = sqlContext.createDataFrame(rdd)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["View the `df` DataFrame generated by the RDD of floats"],"metadata":{}},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["#### Execute the Tensor Graph\nAs noted above, this Tensor graph consists of adding 3 to the tensor created by the `df` DataFrame generated by the RDD of floats.\n* `x` utilizes `tfs.block` where `block` builds a block placeholder based on the content of a column in a dataframe.\n* `z` is a the output tensor from the tensorflow add method (`tf.add`) \n* `df2` is the new DataFrame which adds extra columns to the `df` DataFrame with the `z` tensor block by block"],"metadata":{}},{"cell_type":"code","source":["# Run TensorFlow program executes:\n#   The `op` performs the addition (i.e. `x` + `3`)\n#   Place the data back into a DataFrame\nwith tf.Graph().as_default() as g:\n    # The TensorFlow placeholder that corresponds to column 'x'.\n    # The shape of the placeholder is automatically inferred from the DataFrame.\n    x = tfs.block(df, \"x\")\n    \n    # The output that adds y to x\n    z = tf.add(x, 3, name='z')\n    \n    # The resulting dataframe\n    df2 = tfs.map_blocks(z, df)\n\n# Note that `z` is the tensor output from the `tf.add` operation\nprint z"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["#### Review the output dataframe\nWith the tensor added as a column `z` to the `df` dataframe; you now have the `df2` dataframe that allows you to continue working with your data as a Spark DataFrame."],"metadata":{}},{"cell_type":"code","source":["df2.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Block-wise reducing operations example\nIn this next section, we will show how to work with block-wise reducing operations.  Specifically, we will compute the `sum` and `min` of a field  vectors, working with blocks of rows for more efficient processing.\n\n\n\n#### Building a DataFrame of vectors\nFirst, we will create an one-colummn DataFrame of vectors"],"metadata":{}},{"cell_type":"code","source":["# Build a DataFrame of vectors\ndata = [Row(y=[float(y), float(-y)]) for y in range(10)]\ndf = sqlContext.createDataFrame(data)\ndf.show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Analyze the DataFrame \nWe need to analyze the DataFrame to determine what is its shape (i.e., dimensions of the vectors).  For example, below, we use the `tfs.print_schema` commmand for the `df` DataFrame."],"metadata":{}},{"cell_type":"code","source":["# Print the information gathered by TensorFlow to check the content of the DataFrame\ntfs.print_schema(df)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Notice the `double[?,?]` meaning that TensorFlow does not know the dimensions of the vectors."],"metadata":{}},{"cell_type":"code","source":["# Because the dataframe contains vectors, we need to analyze it first to find the\n# dimensions of the vectors.\ndf2 = tfs.analyze(df)\n\n# The information gathered by TF can be printed to check the content:\ntfs.print_schema(df2)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#### Analyze This\nUpon analysis via `df2` DataFrame, TensorFlow has inferred that `y` contains vectors of size 2.  For small tensors (scalars and vectors), TensorFrames usually infers the shapes of the tensors without requiring a preliminary analysis. If it cannot do it, an error message will indicate that you need to run the DataFrame through `tfs.analyze()` first.\n\n\n### Compute Elementwise Sum and Min of all vectors\nNow, let's use the analyzed dataframe to compute the sum and the elementwise minimum of all the vectors using `tf.reduce_sum` and `tf.reduce_min`.\n* [`tf.reduce_sum`](https://www.tensorflow.org/api_docs/python/math_ops/reduction#reduce_sum): compute the sum of elements across dimensions of a tensor, e.g. if `x = x = [[3, 2, 1], [-1, 2, 1]]` then `tf.reduce_sum(x) ==> 8`.\n* [`tf.reduce_min`](https://www.tensorflow.org/api_docs/python/math_ops/reduction#reduce_min): compute the minimum of elements across dimensions of a tensor, e.g. if `x = x = [[3, 2, 1], [-1, 2, 1]]` then `tf.reduce_min(x) ==> -1`."],"metadata":{}},{"cell_type":"code","source":["# Note: First, let's make a copy of the 'y' column. This will be very cheap in Spark 2.0+\ndf3 = df2.select(df2.y, df2.y.alias(\"z\"))\n\n# Execute the Tensor Graph\nwith tf.Graph().as_default() as g:\n    # The placeholders. Note the special name that end with '_input':\n    y_input = tfs.block(df3, 'y', tf_name=\"y_input\")\n    z_input = tfs.block(df3, 'z', tf_name=\"z_input\")\n    \n    # Perform elementwise sum and minimum \n    y = tf.reduce_sum(y_input, [0], name='y')\n    z = tf.reduce_min(z_input, [0], name='z')\n    \n    # The resulting dataframe\n    (data_sum, data_min) = tfs.reduce_blocks([y, z], df3)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# The final results are numpy arrays:\nprint \"Elementwise sum: %s and minimum: %s \" % (data_sum, data_min)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### Notes:\n* The scoping of the graphs above is important because TensorFrames finds which DataFrame column to feed to TensorFrames based on the placeholders of the graph. \n* It is good practice to keep small graphs when sending them to Spark."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"TensorFrames: Quick Start","notebookId":2815478197173465},"nbformat":4,"nbformat_minor":0}
