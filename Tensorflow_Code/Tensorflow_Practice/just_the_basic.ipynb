{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic about the graph and Linear Regression:\n",
    "- Note: All variable are dropped after each graph run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen data\n",
    "X = np.random.randn(20,2)\n",
    "y = 2*X[:,0] - 5*X[:,1] + 3 + np.random.randn(20,)\n",
    "y = y.reshape(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = tf.constant(X, name='X')\n",
    "y_tensor = tf.constant(y, name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([2,1], -1,1, dtype=tf.float64), name='theta')\n",
    "b = tf.Variable(tf.random_uniform([1], -1, 1, dtype=tf.float64), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "n_epochs = 10000\n",
    "\n",
    "y_pred = tf.add(tf.matmul(X_tensor, W), b, name = 'prediction')\n",
    "error = y_pred - y_tensor\n",
    "loss = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2/20 * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(W, W-lr*gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0: 48.613830\n",
      "loss at iter 1000: 7.238812\n",
      "loss at iter 2000: 6.779519\n",
      "loss at iter 3000: 6.774283\n",
      "loss at iter 4000: 6.774221\n",
      "loss at iter 5000: 6.774221\n",
      "loss at iter 6000: 6.774221\n",
      "loss at iter 7000: 6.774221\n",
      "loss at iter 8000: 6.774221\n",
      "loss at iter 9000: 6.774221\n"
     ]
    }
   ],
   "source": [
    "# perform linear regression\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_epochs):\n",
    "        if(i%1000==0):\n",
    "            print('loss at iter %d: %f'%(i, loss.eval()))\n",
    "        sess.run(training_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0: 46.654586\n",
      "loss at iter 1000: 8.799842\n",
      "loss at iter 2000: 8.373501\n",
      "loss at iter 3000: 8.368555\n",
      "loss at iter 4000: 8.368496\n",
      "loss at iter 5000: 8.368495\n",
      "loss at iter 6000: 8.368495\n",
      "loss at iter 7000: 8.368495\n",
      "loss at iter 8000: 8.368495\n",
      "loss at iter 9000: 8.368495\n"
     ]
    }
   ],
   "source": [
    "# using autodiff\n",
    "gradients_auto = tf.gradients(loss, W)[0]\n",
    "training_op_auto = tf.assign(W, W-lr*gradients_auto)\n",
    "\n",
    "# perform linear regression\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_epochs):\n",
    "        if(i%1000==0):\n",
    "            print('loss at iter %d: %f'%(i, loss.eval()))\n",
    "        sess.run(training_op_auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feed dict:\n",
    "- A placeholder to put value into in run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feed = tf.placeholder(shape=(20,2), dtype=tf.float64, name='X_feed')\n",
    "y_feed = tf.placeholder(shape=(20,1), dtype=tf.float64, name='y_feed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0: 49.712270\n",
      "loss at iter 100: 8.952125\n",
      "loss at iter 200: 8.474871\n",
      "loss at iter 300: 8.469060\n",
      "loss at iter 400: 8.468986\n",
      "loss at iter 500: 8.468985\n",
      "loss at iter 600: 8.468985\n",
      "loss at iter 700: 8.468985\n",
      "loss at iter 800: 8.468985\n",
      "loss at iter 900: 8.468985\n"
     ]
    }
   ],
   "source": [
    "error = tf.add(tf.matmul(X_feed, W),b) - y_feed \n",
    "loss = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = tf.gradients(loss, [W])[0]\n",
    "training_op = tf.assign(W, W-lr*gradients)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_epochs):\n",
    "        if(i%100==0):\n",
    "            print('loss at iter %d: %f'%(i, loss.eval(feed_dict={X_feed: X, y_feed:y})))\n",
    "        sess.run(training_op, feed_dict={X_feed: X, y_feed:y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using an optimizer:\n",
    "- Instead of using your own training op and assign value to W after each epoch manually, we use Opitmizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feed = tf.placeholder(shape=(20,2), dtype=tf.float64, name='X_feed')\n",
    "y_feed = tf.placeholder(shape=(20,1), dtype=tf.float64, name='y_feed')\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([2,1], -1,1, dtype=tf.float64), name='theta')\n",
    "b = tf.Variable(tf.random_uniform([1], -1, 1, dtype=tf.float64), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0: 34.167995\n",
      "loss at iter 100: 1.221681\n",
      "loss at iter 200: 0.618817\n",
      "loss at iter 300: 0.581512\n",
      "loss at iter 400: 0.577819\n",
      "loss at iter 500: 0.577425\n",
      "loss at iter 600: 0.577383\n",
      "loss at iter 700: 0.577378\n",
      "loss at iter 800: 0.577378\n",
      "loss at iter 900: 0.577378\n"
     ]
    }
   ],
   "source": [
    "error = tf.add(tf.matmul(X_feed, W),b) - y_feed \n",
    "loss = tf.reduce_mean(tf.square(error), name='mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_epochs):\n",
    "        if(i%100==0):\n",
    "            print('loss at iter %d: %f'%(i, loss.eval(feed_dict={X_feed: X, y_feed:y})))\n",
    "        sess.run(training_op, feed_dict={X_feed: X, y_feed:y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visulize training loss and graph:\n",
    "- Log loss every epoch using tf.summary\n",
    "- write tf.get_default_graph() to file\n",
    "- visualize them on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_feed = tf.placeholder(shape=(20,2), dtype=tf.float64, name='X_feed')\n",
    "y_feed = tf.placeholder(shape=(20,1), dtype=tf.float64, name='y_feed')\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([2,1], -1,1, dtype=tf.float64), name='theta')\n",
    "b = tf.Variable(tf.random_uniform([1], -1, 1, dtype=tf.float64), name='bias')\n",
    "\n",
    "error = tf.add(tf.matmul(X_feed, W),b) - y_feed \n",
    "loss = tf.reduce_mean(tf.square(error), name='mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_dir = \"tf_logs\"\n",
    "log_dir = \"{}/run-{}/\".format(root_dir, now)\n",
    "\n",
    "# declare loss logger\n",
    "loss_summary = tf.summary.scalar('MSE', loss)\n",
    "# declare file writer and save current default graph\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0: 54.967268\n",
      "loss at iter 100: 1.635727\n",
      "loss at iter 200: 0.639550\n",
      "loss at iter 300: 0.583268\n",
      "loss at iter 400: 0.577996\n",
      "loss at iter 500: 0.577444\n",
      "loss at iter 600: 0.577385\n",
      "loss at iter 700: 0.577378\n",
      "loss at iter 800: 0.577378\n",
      "loss at iter 900: 0.577378\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_epochs):\n",
    "        if(i%100==0):\n",
    "            print('loss at iter %d: %f'%(i, loss.eval(feed_dict={X_feed: X, y_feed:y})))\n",
    "            # get summary string\n",
    "            summary_string = loss_summary.eval(feed_dict={X_feed: X, y_feed:y})\n",
    "            # write current loss to file\n",
    "            file_writer.add_summary(summary_string, i)\n",
    "        sess.run(training_op, feed_dict={X_feed: X, y_feed:y})\n",
    "        \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Name scope and shared variable\n",
    "- Name scope is used to group variables which are belongs to the same computational task\n",
    "- You can create tensor inside a namescope and its name will have prefix which is that namescope\n",
    "- variable_scope acts like namescope but allow you to use shared variable\n",
    "- Shared variables can be create or reuse by calling tf.get_variable()\n",
    "    - Only variables created by tf.get_variable() can be reused\n",
    "    - If want to reuse, define reuse = True at their variable scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_dir = \"log_dirs/run-{}\".format(now)\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('my_var1'):\n",
    "    x = tf.Variable(4, name='x')\n",
    "    y = tf.Variable(5, name='y')\n",
    "    z = tf.add(x,y,name='z')\n",
    "    \n",
    "with tf.name_scope('my_var2'):\n",
    "    t = tf.add(x,y, name='t')\n",
    "    \n",
    "with tf.variable_scope('my_shared'):\n",
    "    shared_m = tf.get_variable(initializer=tf.constant(np.random.randn(3,2)), name='shared_m' )\n",
    "    \n",
    "with tf.variable_scope('my_shared', reuse=True):\n",
    "    shared_t = tf.get_variable(name='shared_m', dtype=tf.float64)\n",
    "    \n",
    "with tf.name_scope('my_var2'):\n",
    "    k = tf.reduce_mean(tf.square(shared_t), name='k')\n",
    "    \n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph()) \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(z.eval())\n",
    "    print(t.eval())\n",
    "    \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
